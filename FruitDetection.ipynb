{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a015f66-976c-423d-a7bc-c9fa72906b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP1\n",
    "Pre-processing data:\n",
    "- Defining the data augmentation settings for the training dataset and the validation dataset\n",
    "- Splitting the dataset w/ a 80:20 ratio\n",
    "- Things that can be changed: Input image size, normalization parameters, split ratio and batch size\n",
    "\n",
    "'''\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "img_size = 224\n",
    "batch_size = 64\n",
    "\n",
    "data_dir = 'archive/fruits-360_dataset/fruits-360/Training'\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomCrop((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "valid_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(data_dir, transform=train_transforms)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b039bb-1467-4b71-b865-febeba20e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# img_path = 'archive/fruits-360_dataset/fruits-360/Training/Apple Braeburn/0_100.jpg'\n",
    "\n",
    "# img = Image.open(img_path)\n",
    "\n",
    "# img_transformed = valid_transforms(img)\n",
    "# plt.imshow(np.transpose(img_transformed.numpy(), (1, 2, 0)))\n",
    "# plt.savefig('output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0a0354-98f0-4766-8484-957147d01043",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP2:\n",
    "- Building a base model using the pre-trained VGG16 model\n",
    "- Adding a trainable linear layer on top of the pre-trained model to perform the disease classification task.\n",
    "'''\n",
    "import torch.nn as nn\n",
    "import sklearn\n",
    "import torchvision.models as models\n",
    "from torchvision.models.vgg import VGG16_Weights\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "'''\n",
    "Creates an instance of the pre-trained VGG16 model using the models.vgg16() function from the torchvision library. \n",
    "The pretrained=True argument loads the weights for the pre-trained model.\n",
    "'''\n",
    "\n",
    "vgg16 = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "'''\n",
    "Freezes the pre-trained layers so that their weights do not get updated during training.\n",
    "'''\n",
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_classes = len(train_dataset.dataset.classes)\n",
    "#sets the num_features to the number of input features for the last layer of the pre-trained VGG16 model's classifier.\n",
    "num_features = vgg16.classifier[-1].in_features\n",
    "#num_features = vgg16.fc.in_features\n",
    "\n",
    "# replaces the last layer of the pre-trained VGG16 model's classifier with a new linear layer\n",
    "vgg16.classifier[-1] = nn.Linear(num_features, num_classes)\n",
    "#vgg16.fc = nn.Linear(num_features, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vgg16.classifier.parameters(), lr=0.001)\n",
    "metric = sklearn.metrics.accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ba72566-c746-4de9-ae54-a41a39ae5c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f43b2de-baf1-4238-a0d5-f2fc53101331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Epoch number:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "847it [05:20,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Train Loss: 0.0233, Train Acc: 0.6014, Valid Loss: 0.7502, Valid Acc: 0.7860\n",
      "Epoch number:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "847it [05:16,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Train Loss: 0.0156, Train Acc: 0.7050, Valid Loss: 0.5330, Valid Acc: 0.8479\n",
      "Epoch number:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "847it [05:16,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/15], Train Loss: 0.0145, Train Acc: 0.7242, Valid Loss: 0.4523, Valid Acc: 0.8668\n",
      "Epoch number:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "847it [05:22,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/15], Train Loss: 0.0141, Train Acc: 0.7326, Valid Loss: 0.4300, Valid Acc: 0.8678\n",
      "Epoch number:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "847it [05:14,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/15], Train Loss: 0.0141, Train Acc: 0.7387, Valid Loss: 0.3888, Valid Acc: 0.8801\n",
      "Epoch number:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "847it [05:16,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/15], Train Loss: 0.0139, Train Acc: 0.7447, Valid Loss: 0.3632, Valid Acc: 0.8886\n",
      "Epoch number:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "847it [05:18,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/15], Train Loss: 0.0139, Train Acc: 0.7479, Valid Loss: 0.3356, Valid Acc: 0.8919\n",
      "Epoch number:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "847it [05:14,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/15], Train Loss: 0.0137, Train Acc: 0.7521, Valid Loss: 0.3501, Valid Acc: 0.8885\n",
      "Epoch number:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "847it [05:15,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/15], Train Loss: 0.0136, Train Acc: 0.7555, Valid Loss: 0.3311, Valid Acc: 0.8929\n",
      "Epoch number:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "847it [05:18,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/15], Train Loss: 0.0138, Train Acc: 0.7603, Valid Loss: 0.3119, Valid Acc: 0.8965\n",
      "Epoch number:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "847it [05:16,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/15], Train Loss: 0.0137, Train Acc: 0.7592, Valid Loss: 0.3441, Valid Acc: 0.8909\n",
      "Epoch number:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "847it [05:17,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/15], Train Loss: 0.0138, Train Acc: 0.7621, Valid Loss: 0.3112, Valid Acc: 0.8995\n",
      "Epoch number:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "847it [05:18,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/15], Train Loss: 0.0135, Train Acc: 0.7663, Valid Loss: 0.3085, Valid Acc: 0.8999\n",
      "Epoch number:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "847it [05:16,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/15], Train Loss: 0.0138, Train Acc: 0.7641, Valid Loss: 0.3157, Valid Acc: 0.8956\n",
      "Epoch number:  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "847it [05:15,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Train Loss: 0.0137, Train Acc: 0.7689, Valid Loss: 0.3558, Valid Acc: 0.8858\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "STEP3\n",
    "- Training the model\n",
    "'''\n",
    "num_epochs = 15\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "vgg16.to(device)\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_accs = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "    \n",
    "    vgg16.train()\n",
    "    print(\"Epoch number: \", epoch)\n",
    "    for i, (images, labels) in tqdm(enumerate(train_loader)):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = vgg16(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_acc += torch.sum(preds == labels.data)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vgg16.eval()\n",
    "        \n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = vgg16(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "    \n",
    "            valid_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            valid_acc += torch.sum(preds == labels.data)\n",
    "    \n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = train_acc / len(train_loader.dataset)\n",
    "    valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "    valid_acc = valid_acc / len(valid_loader.dataset)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_accs.append(valid_acc)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb23ab9a-1934-462b-9735-8a5e5c64c808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSTEP4\\n- Plotting the learning curve\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "STEP4\n",
    "- Plotting the learning curve\n",
    "'''\n",
    "# train_losses = [t.item() for t in train_losses]\n",
    "# valid_losses = [t.item() for t in valid_losses]\n",
    "# train_accs = [t.item() for t in train_accs]\n",
    "# valid_accs = [t.item() for t in valid_accs]\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(train_losses, label='Training Loss')\n",
    "# plt.plot(valid_losses, label='Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training and Validation Losses')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(train_accs, label='Training Accuracy')\n",
    "# plt.plot(valid_accs, label='Validation Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Training and Validation Accuracies')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3e46bf-f84c-4c86-b4cc-8aa12e488814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
